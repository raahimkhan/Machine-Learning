{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:21:46.732104Z",
     "start_time": "2020-11-24T10:21:44.903598Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import operator\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:21:46.763966Z",
     "start_time": "2020-11-24T10:21:46.738041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninstead of running these commands again and again I made pickle files for my ease which I load\\nbelow. However to check the correctness of this function, you can choose to run these calls by\\nuncommenting them\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_negative_path = \"Dataset/train/neg/\"  # path to positive training reviews\n",
    "train_positive_path = \"Dataset/train/pos/\" # path to negative training reviews\n",
    "test_negative_path = \"Dataset/test/neg/\"  # path to positive test reviews\n",
    "test_positive_path = \"Dataset/test/pos/\" # path to negative test reviews\n",
    "trainx = [] # list containing train movie reviews\n",
    "trainy = [] # list containing train labels\n",
    "testx = [] # list containing test movie reviews\n",
    "testy = [] # list containing test labels\n",
    "\n",
    "# function for reading negative and positive reviews into list\n",
    "def readFiles(path, clas, signal):\n",
    "    for file in glob.iglob(path + \"/\" + \"*.txt\"):\n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            data = f.read()\n",
    "            if signal == \"train\":\n",
    "                trainx.append(data)\n",
    "            else:\n",
    "                testx.append(data)\n",
    "            if clas == \"neg\" and signal == \"train\":\n",
    "                trainy.append(0)\n",
    "            elif clas == \"pos\" and signal == \"train\":\n",
    "                trainy.append(1)\n",
    "            elif clas == \"neg\" and signal == \"test\":\n",
    "                testy.append(0)\n",
    "            else:\n",
    "                testy.append(1)\n",
    "\n",
    "\"\"\"\n",
    "instead of running these commands again and again I made pickle files for my ease which I load\n",
    "below. However to check the correctness of this function, you can choose to run these calls by\n",
    "uncommenting them\n",
    "\"\"\"            \n",
    "    \n",
    "# readFiles(train_negative_path, \"neg\", \"train\")\n",
    "# readFiles(train_positive_path, \"pos\", \"train\")\n",
    "# readFiles(train_negative_path, \"neg\", \"test\")\n",
    "# readFiles(train_positive_path, \"pos\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:21:47.193780Z",
     "start_time": "2020-11-24T10:21:46.767922Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading pickle files created for train and test \"un-processed\" data\n",
    "\n",
    "with open('trainx.pkl', 'rb') as f:\n",
    "    trainx = pickle.load(f)\n",
    "    \n",
    "with open('trainy.pkl', 'rb') as f:\n",
    "    trainy = pickle.load(f)\n",
    "    \n",
    "with open('testx.pkl', 'rb') as f:\n",
    "    testx = pickle.load(f)\n",
    "    \n",
    "with open('testy.pkl', 'rb') as f:\n",
    "    testy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:21:47.209743Z",
     "start_time": "2020-11-24T10:21:47.198806Z"
    }
   },
   "outputs": [],
   "source": [
    "# reading stopwords\n",
    "stop_words = [] # list containing stop words\n",
    "stop_words_path = \"Dataset/stop_words.txt\"  # path to stop words\n",
    "readingStopWords = open(stop_words_path, 'r')\n",
    "for word in readingStopWords:\n",
    "    word = word.strip('\\n')\n",
    "    stop_words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:21:47.383910Z",
     "start_time": "2020-11-24T10:21:47.214724Z"
    }
   },
   "outputs": [],
   "source": [
    "# function for preprocessing\n",
    "def preprocess(signal):\n",
    "    if signal == \"train\":\n",
    "        for i in range(len(trainx)):\n",
    "            text = trainx[i]\n",
    "\n",
    "            # lower casing\n",
    "            text = text.lower()\n",
    "\n",
    "            # removing stop words\n",
    "            for word in stop_words:\n",
    "                text = text.replace(\" \" + word + \" \", \" \")\n",
    "\n",
    "            # removing html characters\n",
    "            text = re.sub(re.compile('<.*?>'), '', text)\n",
    "\n",
    "            # removing punctuations\n",
    "            withoutPunct = \"\"\n",
    "            for c in text:\n",
    "                if c not in string.punctuation:\n",
    "                    withoutPunct = withoutPunct + c\n",
    "            text = withoutPunct\n",
    "\n",
    "            trainx[i] = text\n",
    "            \n",
    "    else:\n",
    "        for i in range(len(testx)):\n",
    "            text = testx[i]\n",
    "\n",
    "            # lower casing\n",
    "            text = text.lower()\n",
    "\n",
    "            # removing stop words\n",
    "            for word in stop_words:\n",
    "                text = text.replace(\" \" + word + \" \", \" \")\n",
    "\n",
    "            # removing html characters\n",
    "            text = re.sub(re.compile('<.*?>'), '', text)\n",
    "\n",
    "            # removing punctuations\n",
    "            withoutPunct = \"\"\n",
    "            for c in text:\n",
    "                if c not in string.punctuation:\n",
    "                    withoutPunct = withoutPunct + c\n",
    "            text = withoutPunct\n",
    "\n",
    "            testx[i] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:22:46.388975Z",
     "start_time": "2020-11-24T10:21:47.386893Z"
    }
   },
   "outputs": [],
   "source": [
    "# now preprocessing\n",
    "preprocess(\"train\")\n",
    "preprocess(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:22:47.371210Z",
     "start_time": "2020-11-24T10:22:46.392830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# converting to numpy arrays and visualizing shapes\n",
    "\n",
    "trainx = np.array(trainx)\n",
    "trainy = np.array(trainy)\n",
    "testx = np.array(testx)\n",
    "testy = np.array(testy)\n",
    "    \n",
    "print(trainx.shape)\n",
    "print(trainy.shape)\n",
    "print('')\n",
    "print(testx.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:22:48.664040Z",
     "start_time": "2020-11-24T10:22:47.377194Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this function creates bow and a dict containing bow words along with indexes\n",
    "\"\"\"\n",
    "\n",
    "def createBOW():\n",
    "    bow = set() # set to store bag of words\n",
    "    vocabulary = {} # dictionary to store bow words and their indexes\n",
    "\n",
    "    for i in range(len(trainx)):\n",
    "        current_text = trainx[i]\n",
    "        current_text = current_text.split()\n",
    "        for word in current_text:\n",
    "            bow.add(word)\n",
    "            \n",
    "    print(\"Total distinct words in bow: \", len(bow))\n",
    "    \n",
    "    for loc, word in enumerate(sorted(list(bow))):\n",
    "        vocabulary[word] = loc\n",
    "        \n",
    "    return bow, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:22:51.734528Z",
     "start_time": "2020-11-24T10:22:48.672014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words in bow:  142614\n"
     ]
    }
   ],
   "source": [
    "bow, vocabulary = createBOW()\n",
    "bow = sorted(list(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:22:51.750485Z",
     "start_time": "2020-11-24T10:22:51.737520Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function counts number of occurances of \"w\" in bigdoc[c] were c belongs to one of the two\n",
    "classes i.e. 1 and 0 in this case and w belongs to vocabulary (bag of words) generated in above cells\n",
    "e.g. vocabulary (bow) belongs to [\"example1\", \"example2\", \"example3\"]\n",
    "bigdoc[0] = [\"movie bad\", \"worst was movie\"]\n",
    "bigdoc[1] = [\"movie great\", \"awesome was movie\"]\n",
    "Hence we will count occurance of \"example1\" in bigdoc[0] and bigdoc[1] and so on\n",
    "These occurances will be used in training algorithm of naive bayes\n",
    "\"\"\"\n",
    "def countOccurances(vocabulary):\n",
    "    row, column, value = [], [], []\n",
    "    for index, review in enumerate(trainx):\n",
    "        review = review.split()\n",
    "        counting = dict(Counter(review))\n",
    "        \n",
    "        for token, total in counting.items():\n",
    "            if len(token) > 2:\n",
    "                temp = vocabulary.get(token)\n",
    "                if temp >= 0:\n",
    "                    row.append(index)\n",
    "                    column.append(temp)\n",
    "                    value.append(total)\n",
    "                    \n",
    "    return csr_matrix((value, (row, column)), shape=(len(trainx), len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:23:15.589884Z",
     "start_time": "2020-11-24T10:22:51.753478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 142614)\n"
     ]
    }
   ],
   "source": [
    "occurances = countOccurances(vocabulary)\n",
    "print(occurances.shape)\n",
    "occurances = occurances.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating bigdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:23:15.907198Z",
     "start_time": "2020-11-24T10:23:15.790281Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This helper function creates bigdoc[c] where c belongs to classes in our dataset i.e. 1 and 0 in this case\n",
    "Basically it will append \"d\" for \"d\" belonging to our reviews in class \"c\"\n",
    "The catch is instead of the reviews it will contain feature vector of that review\n",
    "This bigdoc will be used in training algorithm of naive bayes\n",
    "\"\"\"\n",
    "def createBigDoc(occurances):\n",
    "    bigdoc = {}\n",
    "    neg = []\n",
    "    pos = []\n",
    "    for i in range(len(trainy)):\n",
    "        if i <= 12499:\n",
    "            neg.append(occurances[i])\n",
    "        else:\n",
    "            pos.append(occurances[i])\n",
    "            \n",
    "    bigdoc[0] = neg\n",
    "    bigdoc[1] = pos\n",
    "    return bigdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:23:18.153222Z",
     "start_time": "2020-11-24T10:23:15.913129Z"
    }
   },
   "outputs": [],
   "source": [
    "# generating bigdoc\n",
    "\n",
    "bigdoc = createBigDoc(occurances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:23:18.989028Z",
     "start_time": "2020-11-24T10:23:18.164154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(bigdoc[0]))\n",
    "print(len(bigdoc[1]))\n",
    "print(type(bigdoc[0]))\n",
    "print(type(bigdoc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:25:32.071426Z",
     "start_time": "2020-11-24T10:23:18.995011Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of occurances of w in bigdoc[c] i.e. negative classes\n",
    "# takes around 2 minutes to execute\n",
    "\n",
    "negSumList = np.sum(bigdoc[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:27:34.096001Z",
     "start_time": "2020-11-24T10:25:35.282831Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of occurances of w in bigdoc[c] i.e. positive classes\n",
    "# takes around 2 minutes to execute\n",
    "\n",
    "posSumList = np.sum(bigdoc[1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:27:40.634027Z",
     "start_time": "2020-11-24T10:27:37.585153Z"
    }
   },
   "outputs": [],
   "source": [
    "# adding laplace (Add-1) smoothing\n",
    "\n",
    "negSumListSmoothing = negSumList + 1\n",
    "posSumListSmoothing = posSumList + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:27:42.025775Z",
     "start_time": "2020-11-24T10:27:40.678383Z"
    }
   },
   "outputs": [],
   "source": [
    "# this will basically go into the denominator while calculating loglikelihood\n",
    "\n",
    "negSumListSum = np.sum(negSumListSmoothing)\n",
    "posSumListSum = np.sum(posSumListSmoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T21:54:32.851603Z",
     "start_time": "2020-11-08T21:54:32.829609Z"
    }
   },
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:27:51.747343Z",
     "start_time": "2020-11-24T10:27:42.380666Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function trains the naive bayes classifier\n",
    "D : Total number of documents i.e. the reviews\n",
    "C : Total number of classes. Two in this case i.e. positive (1) and negative (0)\n",
    "returns log P(c), log P(w|c) and vocabulary of D\n",
    "\"\"\"\n",
    "\n",
    "def trainNaiveBayes(D, C):\n",
    "    logprior = {}\n",
    "    bigdoc = {}\n",
    "    loglikelihood = {}\n",
    "    \n",
    "    for c in C: # calculating P(c) terms\n",
    "        \n",
    "        Ndoc = len(D) # number of documents in D (25000 in this particular dataset provided to us)\n",
    "        \n",
    "        Nc = 0 # number of documents from D in class c\n",
    "        for labels in trainy:\n",
    "            if labels == c:\n",
    "                Nc = Nc + 1\n",
    "                \n",
    "        logprior[c] = math.log10(Nc / Ndoc)\n",
    "        \n",
    "        V = bow # vocabulary of D\n",
    "        \n",
    "        # now append(d) for d belonging to D with class c to create big doc of class c\n",
    "        # bigdoc already created in above cells\n",
    "        if c == 0:\n",
    "            mybigdoc = negSumList\n",
    "        else:\n",
    "            mybigdoc = posSumList\n",
    "        \n",
    "        for i in range(len(V)): # calculating P(w|c) terms\n",
    "            \n",
    "            # counting occurances of w in bigdoc[c] -> count(w, c)\n",
    "            currentword = V[i]\n",
    "            count_w_c = mybigdoc[i]\n",
    "            \n",
    "            if c == 0:\n",
    "                totalSum = negSumListSum\n",
    "            else:\n",
    "                totalSum = posSumListSum\n",
    "            \n",
    "            # laplace (Add-1) smoothing has already been applied in above cells\n",
    "            loglikelihood[(currentword, c)] = math.log10((count_w_c + 1) / (totalSum))\n",
    "            \n",
    "    return logprior, loglikelihood, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:28:27.357638Z",
     "start_time": "2020-11-24T10:27:51.750338Z"
    }
   },
   "outputs": [],
   "source": [
    "D = trainx\n",
    "C = [0, 1]\n",
    "logprior, loglikelihood, V = trainNaiveBayes(D, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:28:27.526802Z",
     "start_time": "2020-11-24T10:28:27.375110Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is for prediction\n",
    "V : Vocabulary of D i.e. the train data\n",
    "C : Total number of classes. Two in this case i.e. positive (1) and negative (0)\n",
    "testdoc: the review whose class is to be predicted\n",
    "rest are just parameters returned from train function above\n",
    "returns best c\n",
    "\"\"\"\n",
    "\n",
    "def testNaiveBayes(testdoc, logprior, loglikelihood, C, V):\n",
    "    summ = {}\n",
    "    for c in C:\n",
    "        summ[c] = logprior[c]\n",
    "        for word in testdoc:\n",
    "            # filtered out the words not in V in below cell so we dont need to check here\n",
    "            summ[c] = summ[c] + loglikelihood[(word, c)]\n",
    "    return max(summ.items(), key=operator.itemgetter(1))[0] # basically argmaxc sum[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now predicting and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:28:46.691830Z",
     "start_time": "2020-11-24T10:28:27.530749Z"
    }
   },
   "outputs": [],
   "source": [
    "# just processing test data and removing words that are not in our vocabulary\n",
    "\n",
    "processedTestx = [] # this does not contain words not in our vocabulary\n",
    "mySet = set(V)\n",
    "\n",
    "for i in range(len(testx)):\n",
    "    temp = []\n",
    "    testdoc = testx[i]\n",
    "    testdoc = testdoc.split()\n",
    "    for word in testdoc:\n",
    "        if word in mySet:\n",
    "            temp.append(word)\n",
    "    processedTestx.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:28:52.168756Z",
     "start_time": "2020-11-24T10:28:46.773297Z"
    }
   },
   "outputs": [],
   "source": [
    "# now predicting and calculating accuracy\n",
    "\n",
    "acc = 0 # accuracy\n",
    "for i in range(len(processedTestx)):\n",
    "    testdoc = processedTestx[i]\n",
    "    predicted = testNaiveBayes(testdoc, logprior, loglikelihood, C, V)\n",
    "    if predicted == testy[i]:\n",
    "        acc = acc + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:28:52.199404Z",
     "start_time": "2020-11-24T10:28:52.171795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  93.136 %\n"
     ]
    }
   ],
   "source": [
    "acc = acc / len(testy)\n",
    "print(\"Accuracy on test set: \", acc * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T23:08:24.094388Z",
     "start_time": "2020-11-09T23:08:24.082406Z"
    }
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:29:10.256281Z",
     "start_time": "2020-11-24T10:28:52.203455Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating bag-of-words representation using count vectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "Xtrain = vectorizer.fit_transform(trainx)\n",
    "Xtest = vectorizer.transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:29:10.800291Z",
     "start_time": "2020-11-24T10:29:10.260311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "# alpha means smoothing according to docs\n",
    "\n",
    "clf = MultinomialNB(alpha=1)\n",
    "clf.fit(Xtrain, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:29:11.004117Z",
     "start_time": "2020-11-24T10:29:10.808277Z"
    }
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:29:11.097026Z",
     "start_time": "2020-11-24T10:29:11.007404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  93.016 %\n"
     ]
    }
   ],
   "source": [
    "# calculating accuracy\n",
    "\n",
    "print('Accuracy on test set: ', accuracy_score(testy, pred) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T10:29:11.376262Z",
     "start_time": "2020-11-24T10:29:11.105019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11871,   629],\n",
       "       [ 1117, 11383]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating confusion matrix\n",
    "\n",
    "confusion_matrix(testy, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
